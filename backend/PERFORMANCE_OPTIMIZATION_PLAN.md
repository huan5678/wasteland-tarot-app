# Backend æ•ˆèƒ½å„ªåŒ–è¨ˆç•«

## ğŸ“Š ç•¶å‰ç‹€æ³
- **è¨˜æ†¶é«”ä½¿ç”¨**: 750-770MB (ç©©å®šé‹è¡Œ)
- **éƒ¨ç½²å¹³å°**: Zeabur
- **Python ç‰ˆæœ¬**: 3.11
- **ä¸»è¦æ¡†æ¶**: FastAPI + Uvicorn
- **å·¥ä½œé€²ç¨‹æ•¸**: 4 workers

---

## ğŸ¯ å„ªåŒ–ç›®æ¨™
1. å°‡è¨˜æ†¶é«”ä½¿ç”¨é™ä½è‡³ **400-500MB**ï¼ˆç¯€çœ 30-40%ï¼‰
2. æå‡ API å›æ‡‰æ™‚é–“ **20-30%**
3. å„ªåŒ–å•Ÿå‹•æ™‚é–“
4. æ¸›å°‘è³‡æ–™åº«é€£æ¥æ± é–‹éŠ·
5. å„ªåŒ– AI Provider è¨˜æ†¶é«”ä½”ç”¨

---

## ğŸ” è¨˜æ†¶é«”åˆ†æèˆ‡å„ªåŒ–ç­–ç•¥

### 1. **Worker é€²ç¨‹å„ªåŒ–** (æœ€å¤§å½±éŸ¿)
**ç•¶å‰å•é¡Œ**: 4 workers Ã— ~190MB = 760MB

#### æ–¹æ¡ˆ A: æ¸›å°‘ Worker æ•¸é‡ (æ¨è–¦)
```dockerfile
# ä¿®æ”¹ Dockerfile
CMD ["uvicorn", "app.main:app", \
     "--host", "0.0.0.0", \
     "--port", "8000", \
     "--workers", "2", \          # å¾ 4 æ¸›è‡³ 2
     "--log-level", "info"]
```
**é æœŸç¯€çœ**: ~380MB â†’ ç›®æ¨™ 380-400MB

#### æ–¹æ¡ˆ B: ä½¿ç”¨ Gunicorn + Uvicorn Workers
```bash
# æ›´ç²¾ç´°çš„å·¥ä½œé€²ç¨‹ç®¡ç†
gunicorn app.main:app \
  --worker-class uvicorn.workers.UvicornWorker \
  --workers 2 \
  --max-requests 1000 \           # å®šæœŸé‡å•Ÿ worker é‡‹æ”¾è¨˜æ†¶é«”
  --max-requests-jitter 100 \
  --timeout 30
```

### 2. **ä¾è³´å¥—ä»¶å„ªåŒ–**
**ç•¶å‰å•é¡Œ**: éå¤šé‡é‡ç´šä¾è³´

#### ç§»é™¤æœªä½¿ç”¨çš„ä¾è³´
```toml
# pyproject.toml æª¢æŸ¥ä¸¦ç§»é™¤
# è€ƒæ…®ç§»é™¤æˆ–æ›¿æ›:
- redis  # å¦‚æœæ²’ä½¿ç”¨ Redis caching
- prometheus-client  # å¦‚æœæ²’å•Ÿç”¨ç›£æ§
- edge-tts, gtts  # å¦‚æœåªç”¨ä¸€ç¨® TTS
```

#### ä½¿ç”¨è¼•é‡ç´šæ›¿ä»£æ–¹æ¡ˆ
```toml
# æ›¿æ›æ–¹æ¡ˆ
psycopg[binary] â†’ psycopg[binary,pool]  # ä½¿ç”¨é€£æ¥æ± 
anthropic, openai, google-generativeai â†’ æŒ‰éœ€è¼‰å…¥
```

### 3. **è³‡æ–™åº«é€£æ¥æ± å„ªåŒ–**

#### ç•¶å‰é…ç½®æª¢æŸ¥
```python
# app/db/session.py æˆ– database.py
# å„ªåŒ–é€£æ¥æ± è¨­å®š
engine = create_async_engine(
    DATABASE_URL,
    pool_size=5,              # å¾ 10 é™è‡³ 5
    max_overflow=5,           # å¾ 10 é™è‡³ 5
    pool_pre_ping=True,
    pool_recycle=3600,
    echo=False                # ç¢ºä¿ç”Ÿç”¢ç’°å¢ƒé—œé–‰ SQL logging
)
```

### 4. **AI Provider å„ªåŒ–**

#### å»¶é²è¼‰å…¥ (Lazy Loading)
```python
# app/providers/factory.py
class AIProviderFactory:
    _providers = {}
    
    @classmethod
    def get_provider(cls, provider_type: str):
        # åªåœ¨éœ€è¦æ™‚æ‰è¼‰å…¥
        if provider_type not in cls._providers:
            if provider_type == "openai":
                from app.providers.openai_provider import OpenAIProvider
                cls._providers[provider_type] = OpenAIProvider()
        return cls._providers[provider_type]
```

#### å…±äº« HTTP å®¢æˆ¶ç«¯
```python
# é¿å…æ¯å€‹ provider éƒ½å‰µå»º httpx.AsyncClient
import httpx

class SharedHTTPClient:
    _client: httpx.AsyncClient | None = None
    
    @classmethod
    def get_client(cls) -> httpx.AsyncClient:
        if cls._client is None:
            cls._client = httpx.AsyncClient(
                timeout=30.0,
                limits=httpx.Limits(max_keepalive_connections=5)
            )
        return cls._client
```

### 5. **Logging å„ªåŒ–**

#### æ¸›å°‘æ—¥èªŒè¨˜æ†¶é«”ä½”ç”¨
```python
# app/core/logging_config.py
def setup_logging(
    level: str = "INFO",
    max_bytes: int = 10 * 1024 * 1024,  # 10MB
    backup_count: int = 3,               # æ¸›å°‘å‚™ä»½æ•¸é‡
    enable_json: bool = True,
    enable_file: bool = False            # ç”Ÿç”¢ç’°å¢ƒè€ƒæ…®é—œé–‰æ–‡ä»¶æ—¥èªŒ
):
    # ä½¿ç”¨ stdout è€Œéæ–‡ä»¶ï¼Œè®“å¹³å°è™•ç†æ—¥èªŒæ”¶é›†
    pass
```

### 6. **Scheduler å„ªåŒ–**

#### APScheduler è¼•é‡åŒ–é…ç½®
```python
# app/core/scheduler.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.memory import MemoryJobStore  # ä½¿ç”¨è¨˜æ†¶é«”å„²å­˜

jobstores = {
    'default': MemoryJobStore()
}
executors = {
    'default': {'type': 'threadpool', 'max_workers': 2}  # é™åˆ¶åŸ·è¡Œç·’æ•¸
}
```

### 7. **Response Caching**

#### ä½¿ç”¨å…§å»ºå¿«å–è€Œé Redis
```python
from functools import lru_cache
from datetime import datetime, timedelta

class SimpleCache:
    def __init__(self, ttl: int = 300):
        self._cache = {}
        self._ttl = ttl
    
    def get(self, key: str):
        if key in self._cache:
            value, timestamp = self._cache[key]
            if datetime.now() - timestamp < timedelta(seconds=self._ttl):
                return value
            del self._cache[key]
        return None
    
    def set(self, key: str, value):
        self._cache[key] = (value, datetime.now())
```

---

## ğŸ“ å„ªåŒ–å¯¦æ–½æ­¥é©Ÿ

### Phase 1: ç«‹å³å„ªåŒ– (é è¨ˆç¯€çœ 200-300MB)
1. âœ… **æ¸›å°‘ Uvicorn workers è‡³ 2**
   ```bash
   # æ¸¬è©¦å‘½ä»¤
   uvicorn app.main:app --workers 2 --host 0.0.0.0 --port 8000
   ```

2. âœ… **å„ªåŒ–è³‡æ–™åº«é€£æ¥æ± **
   - pool_size: 10 â†’ 5
   - max_overflow: 10 â†’ 5

3. âœ… **é—œé–‰ç”Ÿç”¢ç’°å¢ƒæª”æ¡ˆæ—¥èªŒ**
   - enable_file=False
   - åªä½¿ç”¨ stdout

### Phase 2: ä¾è³´å„ªåŒ– (é è¨ˆç¯€çœ 50-100MB)
1. ğŸ”„ **å¯©æŸ¥ä¸¦ç§»é™¤æœªä½¿ç”¨ä¾è³´**
   ```bash
   # ç”Ÿæˆä¾è³´ä½¿ç”¨å ±å‘Š
   pipdeptree -p wasteland-tarot-backend
   ```

2. ğŸ”„ **AI Provider å»¶é²è¼‰å…¥**
   - å¯¦æ–½ lazy loading pattern
   - å…±äº« HTTP å®¢æˆ¶ç«¯

### Phase 3: é€²éšå„ªåŒ– (é è¨ˆç¯€çœ 50-100MB)
1. ğŸ”„ **å¯¦æ–½æ‡‰ç”¨å±¤å¿«å–**
   - å¡ç‰Œè³‡æ–™å¿«å– (å¾ˆå°‘è®Šå‹•)
   - ç‰Œé™£æ¨¡æ¿å¿«å–
   - æ•…äº‹æ¨¡æ¿å¿«å–

2. ğŸ”„ **å„ªåŒ– Scheduler**
   - ä½¿ç”¨ MemoryJobStore
   - æ¸›å°‘åŸ·è¡Œç·’æ± å¤§å°

3. ğŸ”„ **Response å£“ç¸®**
   ```python
   from fastapi.middleware.gzip import GZipMiddleware
   app.add_middleware(GZipMiddleware, minimum_size=1000)
   ```

### Phase 4: ç›£æ§èˆ‡èª¿æ•´
1. ğŸ“Š **å¯¦æ–½è¨˜æ†¶é«”ç›£æ§**
   ```python
   import psutil
   import os
   
   @app.get("/metrics/memory")
   async def memory_metrics():
       process = psutil.Process(os.getpid())
       return {
           "memory_mb": process.memory_info().rss / 1024 / 1024,
           "memory_percent": process.memory_percent()
       }
   ```

2. ğŸ“Š **æ•ˆèƒ½åŸºæº–æ¸¬è©¦**
   ```bash
   # ä½¿ç”¨ locust é€²è¡Œè² è¼‰æ¸¬è©¦
   locust -f tests/performance/locustfile.py --host=http://localhost:8000
   ```

---

## ğŸ› ï¸ å…·é«”ä»£ç¢¼ä¿®æ”¹

### 1. Dockerfile å„ªåŒ–
```dockerfile
# backend/Dockerfile
FROM python:3.11-slim AS runtime

# ... å…¶ä»–é…ç½® ...

# å„ªåŒ–å•Ÿå‹•å‘½ä»¤
CMD ["uvicorn", "app.main:app", \
     "--host", "0.0.0.0", \
     "--port", "8000", \
     "--workers", "2", \
     "--worker-class", "uvicorn.workers.UvicornWorker", \
     "--log-level", "warning", \
     "--access-log"]
```

### 2. è³‡æ–™åº«é…ç½®å„ªåŒ–
```python
# app/db/session.py
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

engine = create_async_engine(
    settings.database_url,
    echo=False,  # é—œé–‰ SQL logging
    pool_size=5,
    max_overflow=5,
    pool_pre_ping=True,
    pool_recycle=3600,
    pool_timeout=30,
    connect_args={
        "server_settings": {
            "application_name": "wasteland-tarot",
            "jit": "off"  # é—œé–‰ JIT ç·¨è­¯ä»¥ç¯€çœè¨˜æ†¶é«”
        }
    }
)
```

### 3. AI Provider å·¥å» å„ªåŒ–
```python
# app/providers/factory.py
from typing import Dict, Type
import httpx

class OptimizedAIProviderFactory:
    _providers: Dict[str, Any] = {}
    _http_client: httpx.AsyncClient | None = None
    
    @classmethod
    def get_http_client(cls) -> httpx.AsyncClient:
        if cls._http_client is None:
            cls._http_client = httpx.AsyncClient(
                timeout=30.0,
                limits=httpx.Limits(
                    max_keepalive_connections=5,
                    max_connections=10
                )
            )
        return cls._http_client
    
    @classmethod
    def get_provider(cls, provider_type: str):
        if provider_type not in cls._providers:
            # å»¶é²è¼‰å…¥
            if provider_type == "openai":
                from app.providers.openai_provider import OpenAIProvider
                cls._providers[provider_type] = OpenAIProvider(
                    http_client=cls.get_http_client()
                )
            elif provider_type == "gemini":
                from app.providers.gemini_provider import GeminiProvider
                cls._providers[provider_type] = GeminiProvider(
                    http_client=cls.get_http_client()
                )
        return cls._providers[provider_type]
```

### 4. æ‡‰ç”¨å±¤å¿«å–
```python
# app/core/cache.py
from typing import Any, Optional
from datetime import datetime, timedelta
import asyncio

class MemoryCache:
    def __init__(self):
        self._cache: Dict[str, tuple[Any, datetime]] = {}
        self._lock = asyncio.Lock()
    
    async def get(self, key: str, ttl: int = 300) -> Optional[Any]:
        async with self._lock:
            if key in self._cache:
                value, timestamp = self._cache[key]
                if datetime.now() - timestamp < timedelta(seconds=ttl):
                    return value
                del self._cache[key]
            return None
    
    async def set(self, key: str, value: Any):
        async with self._lock:
            self._cache[key] = (value, datetime.now())
    
    async def clear(self):
        async with self._lock:
            self._cache.clear()

# å…¨å±€å¿«å–å¯¦ä¾‹
app_cache = MemoryCache()
```

```python
# app/api/v1/endpoints/cards.py
from app.core.cache import app_cache

@router.get("/cards")
async def get_cards(db: AsyncSession = Depends(get_db)):
    # å˜—è©¦å¾å¿«å–ç²å–
    cache_key = "all_cards"
    cached = await app_cache.get(cache_key, ttl=3600)  # 1å°æ™‚
    
    if cached:
        return cached
    
    # å¾è³‡æ–™åº«æŸ¥è©¢
    cards = await card_service.get_all_cards(db)
    await app_cache.set(cache_key, cards)
    
    return cards
```

---

## ğŸ“ˆ é æœŸæ•ˆæœ

| å„ªåŒ–é …ç›® | è¨˜æ†¶é«”ç¯€çœ | æ•ˆèƒ½æå‡ | å„ªå…ˆç´š |
|---------|----------|---------|--------|
| Worker æ•¸é‡ (4â†’2) | ~380MB | - | â­â­â­ |
| è³‡æ–™åº«é€£æ¥æ±  | ~50MB | +10% | â­â­â­ |
| AI Provider å»¶é²è¼‰å…¥ | ~100MB | +15% | â­â­ |
| æ—¥èªŒå„ªåŒ– | ~30MB | +5% | â­â­ |
| ä¾è³´æ¸…ç† | ~50MB | - | â­â­ |
| æ‡‰ç”¨å¿«å– | ~20MB | +30% | â­â­â­ |
| **ç¸½è¨ˆ** | **~400MB** | **+25-35%** | - |

### å„ªåŒ–å¾Œé æœŸ
- **è¨˜æ†¶é«”ä½¿ç”¨**: 350-400MB (å¾ 750MB)
- **å•Ÿå‹•æ™‚é–“**: <10 ç§’
- **API å¹³å‡å›æ‡‰æ™‚é–“**: <200ms
- **è³‡æ–™åº«é€£æ¥**: ç©©å®šåœ¨ 3-5 å€‹æ´»å‹•é€£æ¥

---

## ğŸ” ç›£æ§èˆ‡é©—è­‰

### è¨˜æ†¶é«”ç›£æ§ç«¯é»
```python
# app/api/v1/endpoints/health.py
import psutil
import os

@router.get("/metrics")
async def get_metrics():
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    
    return {
        "memory": {
            "rss_mb": memory_info.rss / 1024 / 1024,
            "vms_mb": memory_info.vms / 1024 / 1024,
            "percent": process.memory_percent()
        },
        "cpu_percent": process.cpu_percent(interval=1),
        "connections": len(process.connections()),
        "num_threads": process.num_threads()
    }
```

### è² è¼‰æ¸¬è©¦è…³æœ¬
```python
# tests/performance/test_memory.py
import pytest
import psutil
import os

@pytest.mark.performance
async def test_memory_under_load():
    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss / 1024 / 1024
    
    # åŸ·è¡Œ 1000 æ¬¡è«‹æ±‚
    for _ in range(1000):
        response = await client.get("/api/v1/cards")
        assert response.status_code == 200
    
    final_memory = process.memory_info().rss / 1024 / 1024
    memory_growth = final_memory - initial_memory
    
    # ç¢ºä¿è¨˜æ†¶é«”å¢é•·å°æ–¼ 50MB
    assert memory_growth < 50, f"Memory growth too high: {memory_growth}MB"
```

---

## ğŸ“… å¯¦æ–½æ™‚é–“è¡¨

### Week 1: å¿«é€Ÿå„ªåŒ–
- [ ] Day 1-2: Worker æ•¸é‡èª¿æ•´ + æ¸¬è©¦
- [ ] Day 3-4: è³‡æ–™åº«é€£æ¥æ± å„ªåŒ–
- [ ] Day 5: éƒ¨ç½²åˆ° staging ç’°å¢ƒé©—è­‰

### Week 2: æ·±åº¦å„ªåŒ–
- [ ] Day 1-3: AI Provider é‡æ§‹
- [ ] Day 4-5: å¯¦æ–½æ‡‰ç”¨å¿«å–
- [ ] Day 6-7: æ•ˆèƒ½æ¸¬è©¦èˆ‡èª¿æ•´

### Week 3: ç›£æ§èˆ‡ç©©å®š
- [ ] å¯¦æ–½ç›£æ§ç«¯é»
- [ ] è² è¼‰æ¸¬è©¦
- [ ] éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ
- [ ] æŒçºŒç›£æ§ä¸€é€±

---

## ğŸš¨ é¢¨éšªèˆ‡æ³¨æ„äº‹é …

1. **Worker æ•¸é‡æ¸›å°‘**
   - âš ï¸ å¯èƒ½å½±éŸ¿ä¸¦ç™¼è™•ç†èƒ½åŠ›
   - âœ… ç·©è§£: å¯¦æ–½å¿«å–ï¼Œå„ªåŒ–å›æ‡‰æ™‚é–“
   - âœ… ç›£æ§: QPS èˆ‡å›æ‡‰æ™‚é–“

2. **å»¶é²è¼‰å…¥**
   - âš ï¸ é¦–æ¬¡è«‹æ±‚å¯èƒ½è¼ƒæ…¢
   - âœ… ç·©è§£: é ç†±é—œéµ providers
   - âœ… ç›£æ§: å†·å•Ÿå‹•æ™‚é–“

3. **è¨˜æ†¶é«”å¿«å–**
   - âš ï¸ å¯èƒ½é€ æˆè³‡æ–™ä¸ä¸€è‡´
   - âœ… ç·©è§£: åˆç†è¨­å®š TTL
   - âœ… ç›£æ§: å¿«å–å‘½ä¸­ç‡

---

## ğŸ“š åƒè€ƒè³‡æº

- [FastAPI Performance Tips](https://fastapi.tiangolo.com/deployment/concepts/)
- [Uvicorn Deployment Guide](https://www.uvicorn.org/deployment/)
- [SQLAlchemy Connection Pooling](https://docs.sqlalchemy.org/en/20/core/pooling.html)
- [Python Memory Profiling](https://pypi.org/project/memory-profiler/)

---

## âœ… æª¢æŸ¥æ¸…å–®

### å„ªåŒ–å‰
- [ ] è¨˜éŒ„ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨åŸºæº–
- [ ] åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦ä¸¦è¨˜éŒ„çµæœ
- [ ] å‚™ä»½ç•¶å‰é…ç½®

### å„ªåŒ–ä¸­
- [ ] éšæ®µæ€§å¯¦æ–½ï¼Œæ¯æ¬¡æ¸¬è©¦
- [ ] ä¿æŒç›£æ§æ•¸æ“š
- [ ] è¨˜éŒ„æ‰€æœ‰è®Šæ›´

### å„ªåŒ–å¾Œ
- [ ] é©—è­‰è¨˜æ†¶é«”é™è‡³ç›®æ¨™ç¯„åœ
- [ ] ç¢ºèª API æ•ˆèƒ½æœªé™ä½
- [ ] æ›´æ–°æ–‡æª”
- [ ] åœ˜éšŠåŸ¹è¨“

---

**æœ€å¾Œæ›´æ–°**: 2025-11-05
**è² è²¬äºº**: Backend Team
**ç‹€æ…‹**: ğŸ“‹ Planning
