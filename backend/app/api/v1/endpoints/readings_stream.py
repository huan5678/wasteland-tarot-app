"""
AI é©…å‹•çš„å¡”ç¾…è§£è®€ä¸²æµ API ç«¯é»
æä¾› AI ç”Ÿæˆè§£è®€çš„å³æ™‚ä¸²æµ
"""

import asyncio
import json
import logging
import time
from typing import Optional
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.session import get_db
from app.models.wasteland_card import (
    WastelandCard as WastelandCardModel,
    CharacterVoice,
    KarmaAlignment,
    FactionAlignment
)
from app.models.user import User
from app.services.ai_interpretation_service import AIInterpretationService
from app.core.dependencies import get_ai_interpretation_service, get_current_user
from app.config import settings
from app.monitoring.performance import performance_monitor
from sqlalchemy import select

logger = logging.getLogger(__name__)
router = APIRouter()


class StreamInterpretationRequest(BaseModel):
    """ä¸²æµè§£è®€çš„è«‹æ±‚æ¨¡å‹"""
    card_id: str = Field(..., description="å»¢åœŸå¡ç‰Œçš„ ID")
    question: str = Field(..., description="ä½¿ç”¨è€…çš„å•é¡Œ", max_length=500)
    character_voice: CharacterVoice = Field(
        default=CharacterVoice.PIP_BOY,
        description="è§£è®€çš„è§’è‰²è²éŸ³"
    )
    karma_alignment: KarmaAlignment = Field(
        default=KarmaAlignment.NEUTRAL,
        description="æ¥­åŠ›æƒ…å¢ƒ"
    )
    faction_alignment: Optional[FactionAlignment] = Field(
        default=None,
        description="é¸ç”¨çš„æ´¾ç³»å½±éŸ¿"
    )
    position_meaning: Optional[str] = Field(
        default=None,
        description="ç‰Œé™£ä¸­çš„é¸ç”¨ä½ç½®ï¼ˆä¾‹å¦‚ï¼š'éå»'ã€'ç¾åœ¨'ã€'æœªä¾†'ï¼‰"
    )


class StreamMultiCardRequest(BaseModel):
    """ä¸²æµå¤šå¡è§£è®€çš„è«‹æ±‚æ¨¡å‹"""
    card_ids: list[str] = Field(..., description="ç‰Œé™£ä¸­çš„å¡ç‰Œ ID æ¸…å–®")
    question: str = Field(..., description="ä½¿ç”¨è€…çš„å•é¡Œ", max_length=500)
    character_voice: CharacterVoice = Field(
        default=CharacterVoice.PIP_BOY,
        description="è§£è®€çš„è§’è‰²è²éŸ³"
    )
    karma_alignment: KarmaAlignment = Field(
        default=KarmaAlignment.NEUTRAL,
        description="æ¥­åŠ›æƒ…å¢ƒ"
    )
    faction_alignment: Optional[FactionAlignment] = Field(
        default=None,
        description="é¸ç”¨çš„æ´¾ç³»å½±éŸ¿"
    )
    spread_type: str = Field(
        default="three_card",
        description="ç‰Œé™£ä½ˆå±€é¡å‹"
    )


@router.post(
    "/interpretation/stream",
    summary="ä¸²æµå–®å¡è§£è®€",
    description="""
    **å³æ™‚ä¸²æµ AI é©…å‹•çš„å¡”ç¾…å¡ç‰Œè§£è®€ï¼ˆéœ€è¦èªè­‰ï¼‰**

    æ­¤ç«¯é»ç”¢ç”Ÿä¸¦ä¸²æµ Fallout ä¸»é¡Œçš„å¡”ç¾…å¡ç‰Œè§£è®€ï¼Œ
    éš¨è‘— AI ç”Ÿæˆè€Œä¸²æµï¼Œæä¾›å‹•æ…‹ä¸”å¼•äººå…¥å‹çš„ä½¿ç”¨è€…é«”é©—ã€‚

    èªè­‰è¦æ±‚ï¼š
    - **JWT Token**ï¼šéœ€è¦æœ‰æ•ˆçš„ access tokenï¼ˆCookie æˆ– Authorization headerï¼‰
    - **ä½¿ç”¨è€…å¸³è™Ÿ**ï¼šå¿…é ˆç‚ºå·²å•Ÿç”¨ç‹€æ…‹

    åŠŸèƒ½ï¼š
    - **å³æ™‚ä¸²æµ**ï¼šæ–‡å­—éš¨ç”¢ç”Ÿè€Œé¡¯ç¤º
    - **è§’è‰²è²éŸ³**ï¼šPip-Boyã€é¿é›£æ‰€å±…æ°‘ã€è¶…ç´šè®Šç¨®äººç­‰
    - **æ¥­åŠ›æƒ…å¢ƒ**ï¼šå–„è‰¯ã€ä¸­ç«‹æˆ–é‚ªæƒ¡çš„å½±éŸ¿
    - **æ´¾ç³»å½±éŸ¿**ï¼šé‹¼éµå…„å¼Ÿæœƒã€NCRã€å‡±æ’’è»åœ˜çš„è§€é»
    - **ä½ç½®æ„ç¾©**ï¼šç‰Œé™£ä¸­çš„æƒ…å¢ƒ
    - **Timeout ä¿è­·**ï¼š60 ç§’é€¾æ™‚ä¿è­·ï¼ˆå¯é€éç’°å¢ƒè®Šæ•¸é…ç½®ï¼‰

    å›æ‡‰æ ¼å¼ï¼š
    - Server-Sent Eventsï¼ˆSSEï¼‰æ ¼å¼
    - æ¯å€‹å€å¡Šï¼š`data: {text_chunk}\\n\\n`
    - æœ€çµ‚äº‹ä»¶ï¼š`data: [DONE]\\n\\n`
    - éŒ¯èª¤äº‹ä»¶ï¼š`data: [ERROR] {error_message}\\n\\n`

    é©ç”¨æ–¼ï¼š
    - å‹•æ…‹å¡ç‰Œæ­ç¤ºé«”é©—
    - å¼•äººå…¥å‹çš„è¡Œå‹•è£ç½®äº’å‹•
    - å³æ™‚æ•˜äº‹
    """,
    responses={
        200: {
            "description": "ä¸²æµè§£è®€",
            "content": {"text/event-stream": {"example": "data: æ„šè€…ä»£è¡¨...\n\n"}}
        },
        401: {"description": "æœªèªè­‰æˆ– token ç„¡æ•ˆ"},
        404: {"description": "æ‰¾ä¸åˆ°å¡ç‰Œ"},
        503: {"description": "AI æœå‹™ç„¡æ³•ä½¿ç”¨"}
    }
)
async def stream_card_interpretation(
    request: StreamInterpretationRequest,
    db: AsyncSession = Depends(get_db),
    ai_service: AIInterpretationService = Depends(get_ai_interpretation_service),
    current_user: User = Depends(get_current_user)
):
    """ä¸²æµå–®å¡è§£è®€ (éœ€è¦èªè­‰ + Timeout ä¿è­·)"""

    # Log authenticated user for monitoring
    logger.info(
        f"User {current_user.id} ({current_user.name}) starting streaming session for card {request.card_id}"
    )

    # Check if AI service is available
    if not ai_service.is_available():
        raise HTTPException(
            status_code=503,
            detail="AI interpretation service is currently unavailable"
        )

    # Fetch card from database
    result = await db.execute(
        select(WastelandCardModel).where(WastelandCardModel.id == request.card_id)
    )
    card = result.scalar_one_or_none()

    if not card:
        raise HTTPException(status_code=404, detail=f"Card not found: {request.card_id}")

    # ğŸ”§ CRITICAL FIX: Close database connection before streaming
    # Streaming responses keep the connection open, causing pool exhaustion
    await db.close()

    async def generate_stream():
        """Generator function for streaming response with timeout protection and performance monitoring"""
        # Record session start time
        session_start = time.time()
        first_token_time = None
        token_count = 0
        error_occurred = False

        try:
            # Wrap streaming with timeout protection
            async with asyncio.timeout(settings.streaming_timeout):
                # Stream interpretation chunks
                async for chunk in ai_service.generate_interpretation_stream(
                    card=card,
                    character_voice=request.character_voice,
                    question=request.question,
                    karma=request.karma_alignment,
                    faction=request.faction_alignment,
                    position_meaning=request.position_meaning
                ):
                    # Record first token latency on first chunk
                    if first_token_time is None:
                        first_token_time = time.time()
                        first_token_latency_ms = (first_token_time - session_start) * 1000

                        # Record first token latency (non-blocking)
                        performance_monitor.record_first_token_latency(
                            latency_ms=first_token_latency_ms,
                            provider=settings.ai_provider,
                            user_id=str(current_user.id)
                        )

                    # Increment token count
                    token_count += 1

                    # Format as SSE (Server-Sent Events)
                    # Use JSON encoding to handle newlines in chunk
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

                # Send completion signal
                yield "data: [DONE]\n\n"

                # Record streaming completion metrics
                total_duration_ms = (time.time() - session_start) * 1000
                performance_monitor.record_streaming_completion(
                    duration_ms=total_duration_ms,
                    token_count=token_count,
                    provider=settings.ai_provider,
                    user_id=str(current_user.id)
                )

                # Log successful completion
                logger.info(
                    f"Streaming session completed successfully for user {current_user.id}, card {request.card_id} "
                    f"({token_count} tokens in {total_duration_ms:.2f}ms)"
                )

        except asyncio.TimeoutError:
            error_occurred = True
            # Record error metric
            performance_monitor.record_streaming_error(
                error="timeout",
                provider=settings.ai_provider,
                user_id=str(current_user.id)
            )

            # Log timeout event with context
            logger.error(
                f"Streaming timeout after {settings.streaming_timeout}s for user {current_user.id}, "
                f"card {request.card_id}, provider {settings.ai_provider}",
                exc_info=True
            )
            # Send SSE error event
            yield "data: [ERROR] é€£ç·šé€¾æ™‚ï¼Œè«‹é‡æ–°æ•´ç†æˆ–æª¢æŸ¥ç¶²è·¯é€£ç·š\n\n"

        except Exception as e:
            error_occurred = True
            # Record error metric
            performance_monitor.record_streaming_error(
                error=str(e),
                provider=settings.ai_provider,
                user_id=str(current_user.id)
            )

            logger.error(
                f"Error during streaming for user {current_user.id}, card {request.card_id}: {e}",
                exc_info=True
            )
            yield f"data: [ERROR] {str(e)}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        }
    )


@router.post(
    "/interpretation/stream-multi",
    summary="ä¸²æµå¤šå¡ç‰Œé™£è§£è®€",
    description="""
    **ç‚ºç‰Œé™£ä¸­çš„å¤šå¼µå¡ç‰Œä¸²æµ AI è§£è®€ï¼ˆéœ€è¦èªè­‰ï¼‰**

    ç‚ºæ•´å€‹å¡ç‰Œç‰Œé™£ç”¢ç”Ÿé€£è²«çš„è§£è®€ï¼Œ
    éš¨è‘— AI ç”Ÿæˆè€Œå³æ™‚ä¸²æµã€‚

    èªè­‰è¦æ±‚ï¼š
    - **JWT Token**ï¼šéœ€è¦æœ‰æ•ˆçš„ access tokenï¼ˆCookie æˆ– Authorization headerï¼‰
    - **ä½¿ç”¨è€…å¸³è™Ÿ**ï¼šå¿…é ˆç‚ºå·²å•Ÿç”¨ç‹€æ…‹

    åŠŸèƒ½ï¼š
    - **ç‰Œé™£æ„ŸçŸ¥**ï¼šè€ƒæ…®å¡ç‰Œä½ç½®èˆ‡é—œè¯æ€§
    - **æ•˜äº‹æµç¨‹**ï¼šè·¨æ‰€æœ‰å¡ç‰Œè¬›è¿°å®Œæ•´æ•…äº‹
    - **è§’è‰²è²éŸ³**ï¼šä¿æŒä¸€è‡´çš„å€‹æ€§
    - **æ¥­åŠ›èˆ‡æ´¾ç³»æƒ…å¢ƒ**ï¼šè²«ç©¿æ•´å€‹è§£è®€
    - **Timeout ä¿è­·**ï¼š60 ç§’é€¾æ™‚ä¿è­·ï¼ˆå¯é€éç’°å¢ƒè®Šæ•¸é…ç½®ï¼‰

    ç‰Œé™£é¡å‹ï¼š
    - `three_card`ï¼šéå»ã€ç¾åœ¨ã€æœªä¾†
    - `celtic_cross`ï¼š10 å¼µå¡ç‰Œå…¨é¢å åœ
    - `relationship`ï¼šä½ ã€å°æ–¹ã€é—œä¿‚
    - `decision`ï¼šé¸é …èˆ‡çµæœ

    å›æ‡‰æ ¼å¼ï¼š
    - Server-Sent Eventsï¼ˆSSEï¼‰æ ¼å¼
    - æ¯å€‹å€å¡Šï¼š`data: {text_chunk}\\n\\n`
    - æœ€çµ‚äº‹ä»¶ï¼š`data: [DONE]\\n\\n`
    - éŒ¯èª¤äº‹ä»¶ï¼š`data: [ERROR] {error_message}\\n\\n`
    """,
    responses={
        200: {
            "description": "ä¸²æµå¤šå¡è§£è®€",
            "content": {"text/event-stream": {"example": "data: æª¢è¦–æ‚¨çš„ç‰Œé™£...\n\n"}}
        },
        401: {"description": "æœªèªè­‰æˆ– token ç„¡æ•ˆ"},
        404: {"description": "æ‰¾ä¸åˆ°ä¸€å¼µæˆ–å¤šå¼µå¡ç‰Œ"},
        503: {"description": "AI æœå‹™ç„¡æ³•ä½¿ç”¨"}
    }
)
async def stream_multi_card_interpretation(
    request: StreamMultiCardRequest,
    db: AsyncSession = Depends(get_db),
    ai_service: AIInterpretationService = Depends(get_ai_interpretation_service),
    current_user: User = Depends(get_current_user)
):
    """ä¸²æµå¤šå¡ç‰Œé™£è§£è®€ (éœ€è¦èªè­‰ + Timeout ä¿è­·)"""

    # Log authenticated user for monitoring
    logger.info(
        f"User {current_user.id} ({current_user.name}) starting multi-card streaming session for {len(request.card_ids)} cards"
    )

    # Check if AI service is available
    if not ai_service.is_available():
        raise HTTPException(
            status_code=503,
            detail="AI interpretation service is currently unavailable"
        )

    # Validate we have cards
    if not request.card_ids or len(request.card_ids) == 0:
        raise HTTPException(status_code=400, detail="No cards provided")

    # Fetch all cards from database
    result = await db.execute(
        select(WastelandCardModel).where(WastelandCardModel.id.in_(request.card_ids))
    )
    cards = result.scalars().all()

    if len(cards) != len(request.card_ids):
        raise HTTPException(
            status_code=404,
            detail=f"Some cards not found. Expected {len(request.card_ids)}, found {len(cards)}"
        )

    # Maintain order of cards as requested
    # Convert UUID to string for consistent key matching
    card_map = {str(card.id): card for card in cards}
    ordered_cards = [card_map[card_id] for card_id in request.card_ids]

    # ğŸ”§ CRITICAL FIX: Close database connection before streaming
    # Streaming responses keep the connection open, causing pool exhaustion
    await db.close()

    async def generate_stream():
        """Generator function for streaming response with timeout protection and performance monitoring"""
        # Record session start time
        session_start = time.time()
        first_token_time = None
        token_count = 0
        error_occurred = False

        try:
            # Wrap streaming with timeout protection
            async with asyncio.timeout(settings.streaming_timeout):
                # Stream multi-card interpretation chunks
                async for chunk in ai_service.generate_multi_card_interpretation_stream(
                    cards=ordered_cards,
                    character_voice=request.character_voice,
                    question=request.question,
                    karma=request.karma_alignment,
                    faction=request.faction_alignment,
                    spread_type=request.spread_type
                ):
                    # Record first token latency on first chunk
                    if first_token_time is None:
                        first_token_time = time.time()
                        first_token_latency_ms = (first_token_time - session_start) * 1000

                        # Record first token latency (non-blocking)
                        performance_monitor.record_first_token_latency(
                            latency_ms=first_token_latency_ms,
                            provider=settings.ai_provider,
                            user_id=str(current_user.id)
                        )

                    # Increment token count
                    token_count += 1

                    # Format as SSE
                    # Use JSON encoding to handle newlines in chunk
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

                # Send completion signal
                yield "data: [DONE]\n\n"

                # Record streaming completion metrics
                total_duration_ms = (time.time() - session_start) * 1000
                performance_monitor.record_streaming_completion(
                    duration_ms=total_duration_ms,
                    token_count=token_count,
                    provider=settings.ai_provider,
                    user_id=str(current_user.id)
                )

                # Log successful completion
                logger.info(
                    f"Multi-card streaming session completed successfully for user {current_user.id}, "
                    f"{len(request.card_ids)} cards ({token_count} tokens in {total_duration_ms:.2f}ms)"
                )

        except asyncio.TimeoutError:
            error_occurred = True
            # Record error metric
            performance_monitor.record_streaming_error(
                error="timeout",
                provider=settings.ai_provider,
                user_id=str(current_user.id)
            )

            # Log timeout event with context
            logger.error(
                f"Multi-card streaming timeout after {settings.streaming_timeout}s for user {current_user.id}, "
                f"cards {request.card_ids}, provider {settings.ai_provider}",
                exc_info=True
            )
            # Send SSE error event
            yield "data: [ERROR] é€£ç·šé€¾æ™‚ï¼Œè«‹é‡æ–°æ•´ç†æˆ–æª¢æŸ¥ç¶²è·¯é€£ç·š\n\n"

        except Exception as e:
            error_occurred = True
            # Record error metric
            performance_monitor.record_streaming_error(
                error=str(e),
                provider=settings.ai_provider,
                user_id=str(current_user.id)
            )

            logger.error(
                f"Error during multi-card streaming for user {current_user.id}, {len(request.card_ids)} cards: {e}",
                exc_info=True
            )
            yield f"data: [ERROR] {str(e)}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )
