"""
Phase 5: äººå·¥å“è³ªè©•ä¼° - çµæœåˆ†æè…³æœ¬

åˆ†ææ¸¬è©¦çµæœï¼Œç”Ÿæˆå“è³ªå ±å‘Š
"""
import csv
import json
from pathlib import Path
from typing import Dict, List
from datetime import datetime
import statistics


def load_test_results(results_dir: Path) -> List[Dict]:
    """è¼‰å…¥æ‰€æœ‰æ¸¬è©¦çµæœ"""
    summary_file = results_dir / "test_summary.csv"

    if not summary_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°æ¸¬è©¦ç¸½çµæª”æ¡ˆï¼š{summary_file}")
        return []

    results = []
    with open(summary_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # å°‡è©•åˆ†è½‰æ›ç‚ºæ•¸å­—
            try:
                row['score_psychology'] = int(row['score_psychology']) if row['score_psychology'] else None
                row['score_fallout'] = int(row['score_fallout']) if row['score_fallout'] else None
                row['score_character'] = int(row['score_character']) if row['score_character'] else None
                row['score_usefulness'] = int(row['score_usefulness']) if row['score_usefulness'] else None
                row['score_overall'] = int(row['score_overall']) if row['score_overall'] else None
            except ValueError:
                pass  # ä¿æŒç‚ºå­—ä¸²æˆ– None

            results.append(row)

    return results


def calculate_statistics(results: List[Dict]) -> Dict:
    """è¨ˆç®—çµ±è¨ˆè³‡æ–™"""

    # éæ¿¾æœ‰è©•åˆ†çš„çµæœ
    scored_results = [
        r for r in results
        if all([
            r.get('score_psychology'),
            r.get('score_fallout'),
            r.get('score_character'),
            r.get('score_usefulness'),
            r.get('score_overall')
        ])
    ]

    if not scored_results:
        return {
            "total_tests": len(results),
            "scored_tests": 0,
            "error": "No scored results found. Please fill in the scores in test_summary.csv"
        }

    # è¨ˆç®—ç¸½åˆ†ï¼ˆ5 å€‹ç¶­åº¦ï¼‰
    total_scores = []
    for r in scored_results:
        total = (
            r['score_psychology'] +
            r['score_fallout'] +
            r['score_character'] +
            r['score_usefulness'] +
            r['score_overall']
        )
        r['total_score'] = total
        total_scores.append(total)

    # åŸºæœ¬çµ±è¨ˆ
    stats = {
        "total_tests": len(results),
        "scored_tests": len(scored_results),
        "success_rate": len(scored_results) / len(results) * 100 if results else 0,

        # ç¸½åˆ†çµ±è¨ˆï¼ˆæ»¿åˆ† 25ï¼‰
        "mean_total_score": statistics.mean(total_scores),
        "median_total_score": statistics.median(total_scores),
        "stdev_total_score": statistics.stdev(total_scores) if len(total_scores) > 1 else 0,
        "min_total_score": min(total_scores),
        "max_total_score": max(total_scores),

        # é«˜åˆ†ç‡ï¼ˆâ‰¥ 20/25ï¼‰
        "high_score_count": sum(1 for s in total_scores if s >= 20),
        "high_score_rate": sum(1 for s in total_scores if s >= 20) / len(total_scores) * 100,

        # ä½åˆ†ç‡ï¼ˆ< 15/25ï¼‰
        "low_score_count": sum(1 for s in total_scores if s < 15),
        "low_score_rate": sum(1 for s in total_scores if s < 15) / len(total_scores) * 100,

        # å„ç¶­åº¦å¹³å‡åˆ†
        "avg_psychology": statistics.mean([r['score_psychology'] for r in scored_results]),
        "avg_fallout": statistics.mean([r['score_fallout'] for r in scored_results]),
        "avg_character": statistics.mean([r['score_character'] for r in scored_results]),
        "avg_usefulness": statistics.mean([r['score_usefulness'] for r in scored_results]),
        "avg_overall": statistics.mean([r['score_overall'] for r in scored_results]),
    }

    # è§’è‰²åˆ†çµ„çµ±è¨ˆ
    character_stats = {}
    for character in set(r['character'] for r in scored_results if r.get('character')):
        char_results = [r for r in scored_results if r['character'] == character]
        char_scores = [r['total_score'] for r in char_results]

        character_stats[character] = {
            "count": len(char_results),
            "mean_score": statistics.mean(char_scores),
            "min_score": min(char_scores),
            "max_score": max(char_scores),
        }

    stats['character_stats'] = character_stats

    # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®æ¡ˆä¾‹
    sorted_results = sorted(scored_results, key=lambda r: r['total_score'], reverse=True)
    stats['top_5'] = sorted_results[:5]
    stats['bottom_3'] = sorted_results[-3:]

    return stats


def generate_report(stats: Dict, output_file: Path):
    """ç”Ÿæˆ Markdown å ±å‘Š"""

    if "error" in stats:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# Phase 5: æ¸¬è©¦çµæœå ±å‘Š\n\n")
            f.write(f"âŒ **éŒ¯èª¤**ï¼š{stats['error']}\n\n")
            f.write(f"æ¸¬è©¦ç¸½æ•¸ï¼š{stats['total_tests']}\n")
            f.write(f"å·²è©•åˆ†ï¼š{stats['scored_tests']}\n")
        return

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# Phase 5: äººå·¥å“è³ªè©•ä¼°çµæœå ±å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # åŸ·è¡Œæ‘˜è¦
        f.write("## åŸ·è¡Œæ‘˜è¦\n\n")
        f.write(f"- **æ¸¬è©¦ç¸½æ•¸**ï¼š{stats['total_tests']}\n")
        f.write(f"- **æˆåŠŸåŸ·è¡Œ**ï¼š{stats['scored_tests']} ({stats['success_rate']:.1f}%)\n")
        f.write(f"- **å¹³å‡ç¸½åˆ†**ï¼š{stats['mean_total_score']:.1f}/25 ({stats['mean_total_score']/25*100:.1f}%)\n")
        f.write(f"- **ä¸­ä½æ•¸**ï¼š{stats['median_total_score']}/25\n")
        f.write(f"- **æ¨™æº–å·®**ï¼š{stats['stdev_total_score']:.2f}\n")
        f.write(f"- **åˆ†æ•¸ç¯„åœ**ï¼š{stats['min_total_score']}-{stats['max_total_score']}\n\n")

        # æˆåŠŸæ¨™æº–æª¢æŸ¥
        f.write("## æˆåŠŸæ¨™æº–æª¢æŸ¥\n\n")

        criteria = [
            ("å¹³å‡åˆ†æ•¸ â‰¥ 18/25 (72%)", stats['mean_total_score'] >= 18),
            ("é«˜åˆ†ç‡ â‰¥ 40% (â‰¥20åˆ†)", stats['high_score_rate'] >= 40),
            ("ä½åˆ†ç‡ â‰¤ 10% (<15åˆ†)", stats['low_score_rate'] <= 10),
        ]

        for criterion, passed in criteria:
            icon = "âœ…" if passed else "âŒ"
            f.write(f"- {icon} {criterion}\n")

        all_passed = all(passed for _, passed in criteria)
        f.write(f"\n**æ•´é«”è©•ä¼°**ï¼š{'âœ… é€šéæ‰€æœ‰æ¨™æº–' if all_passed else 'âš ï¸ éƒ¨åˆ†æ¨™æº–æœªé”æˆ'}\n\n")

        # åˆ†æ•¸åˆ†å¸ƒ
        f.write("## åˆ†æ•¸åˆ†å¸ƒ\n\n")
        f.write(f"- **é«˜åˆ†æ¡ˆä¾‹** (â‰¥20åˆ†)ï¼š{stats['high_score_count']} ({stats['high_score_rate']:.1f}%)\n")
        f.write(f"- **ä¸­ç­‰æ¡ˆä¾‹** (15-19åˆ†)ï¼š{stats['scored_tests'] - stats['high_score_count'] - stats['low_score_count']}\n")
        f.write(f"- **ä½åˆ†æ¡ˆä¾‹** (<15åˆ†)ï¼š{stats['low_score_count']} ({stats['low_score_rate']:.1f}%)\n\n")

        # å„ç¶­åº¦å¹³å‡åˆ†
        f.write("## å„ç¶­åº¦å¹³å‡åˆ†\n\n")
        f.write("| ç¶­åº¦ | å¹³å‡åˆ† | ç™¾åˆ†æ¯” |\n")
        f.write("|------|--------|--------|\n")
        f.write(f"| æ¦®æ ¼å¿ƒç†å­¸æ·±åº¦ | {stats['avg_psychology']:.2f}/5 | {stats['avg_psychology']/5*100:.1f}% |\n")
        f.write(f"| Fallout ä¸–ç•Œè§€ | {stats['avg_fallout']:.2f}/5 | {stats['avg_fallout']/5*100:.1f}% |\n")
        f.write(f"| è§’è‰²å€‹æ€§é®®æ˜åº¦ | {stats['avg_character']:.2f}/5 | {stats['avg_character']/5*100:.1f}% |\n")
        f.write(f"| è§£è®€å¯¦ç”¨æ€§ | {stats['avg_usefulness']:.2f}/5 | {stats['avg_usefulness']/5*100:.1f}% |\n")
        f.write(f"| æ•´é«”æ»¿æ„åº¦ | {stats['avg_overall']:.2f}/5 | {stats['avg_overall']/5*100:.1f}% |\n\n")

        # è§’è‰²è¡¨ç¾
        f.write("## è§’è‰²è¡¨ç¾\n\n")
        f.write("| è§’è‰² | æ¸¬è©¦æ•¸ | å¹³å‡ç¸½åˆ† | åˆ†æ•¸ç¯„åœ |\n")
        f.write("|------|--------|----------|----------|\n")

        for character, char_stats in sorted(
            stats['character_stats'].items(),
            key=lambda x: x[1]['mean_score'],
            reverse=True
        ):
            f.write(f"| {character} | {char_stats['count']} | ")
            f.write(f"{char_stats['mean_score']:.1f}/25 | ")
            f.write(f"{char_stats['min_score']}-{char_stats['max_score']} |\n")

        f.write("\n")

        # Top 5 æ¡ˆä¾‹
        f.write("## ğŸ† Top 5 å„ªç§€æ¡ˆä¾‹\n\n")
        for i, result in enumerate(stats['top_5'], 1):
            f.write(f"### {i}. {result['test_id']} - ç¸½åˆ† {result['total_score']}/25\n\n")
            f.write(f"- **ç‰Œå¡**ï¼š{result['card_name']}\n")
            f.write(f"- **è§’è‰²**ï¼š{result['character']}\n")
            f.write(f"- **å•é¡Œé¡å‹**ï¼š{result['question_type']}\n")
            f.write(f"- **è©•åˆ†**ï¼šå¿ƒç†å­¸ {result['score_psychology']}, ")
            f.write(f"Fallout {result['score_fallout']}, ")
            f.write(f"è§’è‰² {result['score_character']}, ")
            f.write(f"å¯¦ç”¨æ€§ {result['score_usefulness']}, ")
            f.write(f"æ•´é«” {result['score_overall']}\n")
            if result.get('comments'):
                f.write(f"- **è©•èª**ï¼š{result['comments']}\n")
            f.write("\n")

        # Bottom 3 æ¡ˆä¾‹
        f.write("## âš ï¸ éœ€æ”¹é€²æ¡ˆä¾‹ (Bottom 3)\n\n")
        for i, result in enumerate(stats['bottom_3'], 1):
            f.write(f"### {i}. {result['test_id']} - ç¸½åˆ† {result['total_score']}/25\n\n")
            f.write(f"- **ç‰Œå¡**ï¼š{result['card_name']}\n")
            f.write(f"- **è§’è‰²**ï¼š{result['character']}\n")
            f.write(f"- **å•é¡Œé¡å‹**ï¼š{result['question_type']}\n")
            f.write(f"- **è©•åˆ†**ï¼šå¿ƒç†å­¸ {result['score_psychology']}, ")
            f.write(f"Fallout {result['score_fallout']}, ")
            f.write(f"è§’è‰² {result['score_character']}, ")
            f.write(f"å¯¦ç”¨æ€§ {result['score_usefulness']}, ")
            f.write(f"æ•´é«” {result['score_overall']}\n")
            if result.get('comments'):
                f.write(f"- **è©•èª**ï¼š{result['comments']}\n")
            f.write("\n")

        # å»ºè­°è¡Œå‹•
        f.write("## å»ºè­°è¡Œå‹•é …ç›®\n\n")

        if all_passed:
            f.write("âœ… **æ‰€æœ‰å“è³ªæ¨™æº–å·²é”æˆï¼**\n\n")
            f.write("å»ºè­°è¡Œå‹•ï¼š\n")
            f.write("1. ç¹¼çºŒç›£æ§å¯¦éš›ä½¿ç”¨ä¸­çš„å“è³ª\n")
            f.write("2. æ”¶é›†ç”¨æˆ¶åé¥‹é€²ä¸€æ­¥å„ªåŒ–\n")
            f.write("3. è€ƒæ…®é€²è¡Œ A/B æ¸¬è©¦é©—è­‰æ”¹é€²æ•ˆæœ\n")
        else:
            f.write("âš ï¸ **éƒ¨åˆ†æ¨™æº–æœªé”æˆï¼Œå»ºè­°æ¡å–ä»¥ä¸‹è¡Œå‹•ï¼š**\n\n")

            if stats['mean_total_score'] < 18:
                f.write("1. **æå‡æ•´é«”å“è³ª**ï¼šå¹³å‡åˆ†ä½æ–¼ 18ï¼Œå»ºè­°æª¢è¦–ä½åˆ†æ¡ˆä¾‹ä¸¦å„ªåŒ– Prompt\n")

            if stats['high_score_rate'] < 40:
                f.write("2. **æé«˜é«˜åˆ†ç‡**ï¼šå„ªç§€æ¡ˆä¾‹ä¸è¶³ 40%ï¼Œå»ºè­°å¼·åŒ–å¿ƒç†å­¸æ·±åº¦å’Œè§’è‰²å€‹æ€§\n")

            if stats['low_score_rate'] > 10:
                f.write("3. **æ¸›å°‘ä½åˆ†æ¡ˆä¾‹**ï¼šä½åˆ†æ¡ˆä¾‹è¶…é 10%ï¼Œå»ºè­°é‡å°æ€§æ”¹é€²åº•éƒ¨ 3 å€‹æ¡ˆä¾‹\n")

            # è§’è‰²ç‰¹å®šå»ºè­°
            weak_characters = [
                char for char, char_stats in stats['character_stats'].items()
                if char_stats['mean_score'] < 15
            ]

            if weak_characters:
                f.write(f"\n4. **æ”¹é€²å¼±å‹¢è§’è‰²**ï¼šä»¥ä¸‹è§’è‰²å¹³å‡åˆ†ä½æ–¼ 15ï¼Œå»ºè­°å„ªåŒ– Promptï¼š\n")
                for char in weak_characters:
                    f.write(f"   - {char}\n")

        f.write("\n")

        # çµè«–
        f.write("## çµè«–\n\n")
        f.write(f"æœ¬æ¬¡æ¸¬è©¦å…±åŸ·è¡Œ {stats['scored_tests']} å€‹æ¡ˆä¾‹ï¼Œ")
        f.write(f"å¹³å‡ç¸½åˆ†ç‚º {stats['mean_total_score']:.1f}/25 ({stats['mean_total_score']/25*100:.1f}%)ã€‚\n\n")

        if all_passed:
            f.write("**âœ… æ–°ç‰ˆ Character Prompt å“è³ªå„ªè‰¯ï¼Œé”åˆ°é æœŸç›®æ¨™ã€‚**\n")
        else:
            f.write("**âš ï¸ æ–°ç‰ˆ Character Prompt ä»æœ‰æ”¹é€²ç©ºé–“ï¼Œå»ºè­°é‡å°å¼±é …é€²è¡Œå„ªåŒ–ã€‚**\n")


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""

    print("=" * 80)
    print("Phase 5: çµæœåˆ†æ")
    print("=" * 80)
    print()

    # å°‹æ‰¾çµæœç›®éŒ„
    results_dir = Path(__file__).parent.parent / ".kiro/specs/refactor-tarot-system-prompt/phase5_results"

    if not results_dir.exists():
        print(f"âŒ æ‰¾ä¸åˆ°çµæœç›®éŒ„ï¼š{results_dir}")
        print("è«‹å…ˆåŸ·è¡Œï¼špython scripts/execute_phase5_manual_test.py")
        return

    # è¼‰å…¥çµæœ
    print("ğŸ“‚ è¼‰å…¥æ¸¬è©¦çµæœ...")
    results = load_test_results(results_dir)

    if not results:
        print("âŒ ç„¡æ³•è¼‰å…¥æ¸¬è©¦çµæœ")
        return

    print(f"âœ… è¼‰å…¥ {len(results)} å€‹æ¸¬è©¦çµæœ")
    print()

    # è¨ˆç®—çµ±è¨ˆ
    print("ğŸ“Š è¨ˆç®—çµ±è¨ˆè³‡æ–™...")
    stats = calculate_statistics(results)

    if "error" in stats:
        print(f"âš ï¸ {stats['error']}")
        print("è«‹åœ¨ test_summary.csv ä¸­å¡«å¯«è©•åˆ†å¾Œå†åŸ·è¡Œæ­¤è…³æœ¬")
    else:
        print(f"âœ… çµ±è¨ˆå®Œæˆ")
        print(f"   å¹³å‡ç¸½åˆ†ï¼š{stats['mean_total_score']:.1f}/25")
        print(f"   é«˜åˆ†ç‡ï¼š{stats['high_score_rate']:.1f}%")
        print(f"   ä½åˆ†ç‡ï¼š{stats['low_score_rate']:.1f}%")

    print()

    # ç”Ÿæˆå ±å‘Š
    report_file = results_dir / "PHASE5_TEST_RESULTS.md"
    print(f"ğŸ“ ç”Ÿæˆå ±å‘Šï¼š{report_file}")
    generate_report(stats, report_file)

    print("âœ… å ±å‘Šç”Ÿæˆå®Œæˆï¼")
    print()
    print(f"è«‹æŸ¥çœ‹ï¼š{report_file}")


if __name__ == "__main__":
    main()
